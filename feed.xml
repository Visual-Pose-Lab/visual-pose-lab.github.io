<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://visual-pose-lab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://visual-pose-lab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-03T08:27:18+00:00</updated><id>https://visual-pose-lab.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://visual-pose-lab.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://visual-pose-lab.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://visual-pose-lab.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">AdaptivePose</title><link href="https://visual-pose-lab.github.io/blog/2023/adaptive-pose/" rel="alternate" type="text/html" title="AdaptivePose"/><published>2023-08-30T20:40:16+00:00</published><updated>2023-08-30T20:40:16+00:00</updated><id>https://visual-pose-lab.github.io/blog/2023/adaptive-pose</id><content type="html" xml:base="https://visual-pose-lab.github.io/blog/2023/adaptive-pose/"><![CDATA[<h1 id="adaptivepose-human-parts-as-adaptive-pointsaaai-2022">AdaptivePose: Human Parts as Adaptive Points(AAAI 2022)</h1> <h1 id="adaptivepose-a-powerful-single-stage-network-for-multi-person-pose-regression">AdaptivePose++: A Powerful Single-Stage Network for Multi-Person Pose Regression</h1> <h2 id="adaptive-point-自适应点">adaptive point 自适应点</h2> <p><img src="assets/img/attachment/AdaptivePoints20230730180308.png" alt="AdaptivePoints"/></p> <ul> <li>(a) 传统的表示方法：top-down&amp;bottom-up</li> <li>(b) CenterNet提出的center-to-joint身体表示</li> <li>(c) SPM提出的分等级身体表示法</li> <li>(d) adaptIve points 对比结果(网络模型不相同)：</li> </ul> <div align="center"> <img src="assets/img/attachment/AdaptivePoseComparison20230730182635.png" width="50%" height="50%" align="center"/> </div> <p>1) 提出的点集表示在自适应部位相关点处引入了额外的特征，与有限的中心表示相比，这些特征能够编码更有信息的特征，用于灵活的姿态。The proposed point set representation introduces additional features at adaptive part related points, which are able to encode more informative features for flexible pose compared with limited center representation. 2) 自适应部分相关点作为中继节点，可以在单次前向传递中更有效地建模人类实例与相应关键点之间的关联。The adaptive part related points serves as relay nodes can more effectively model the associations between human instance and corresponding keypoints in a single-forward pass.</p> <h2 id="three-parts">Three Parts</h2> <ul> <li>The <strong>Part Perception Module</strong> to regress seven adaptive human-part related points for perceiving corresponding seven human parts.</li> <li>In contrast to using the limited feature with fixed receptive field to predict the human center, the <strong>Enhanced Centeraware Branch</strong> conducts the receptive field adaptation by aggregating the features of adaptive human-part related points to perceive the center of various pose more precisely.</li> <li>The <strong>Two-hop Regression Branch</strong> together with the <strong>Skeleton-Aware Regression Loss</strong> for regressing keypoints. The adaptive human-part related points act as one-hop nodes to factorize the center-to-joint offsets dynamically.</li> </ul> <h2 id="five-aspects-from-adaptivepose-to-adaptivepose">Five Aspects from AdaptivePose to AdaptivePose++</h2> <ul> <li>more details for clearer and more comprehensive presentation</li> <li>improve the regression loss and add an additional loss term to learn the bone connections of inner parts and cross parts, which is helpful for crowd scene</li> <li>more experiments</li> <li>sota on Crowdpose</li> <li>extent the method to 3D multi-person pose estimation task</li> </ul> <h2 id="contributions">Contributions</h2> <ul> <li>We propose to represent human parts as points thus the human body can be represented via an adaptive point set including center and several human-part related points. To our best knowledge, we are the first to present a finegained and adaptive body representation to sufficiently encode the pose information and effectively build up the relation between the human instance and keypoints in a single-forward pass.</li> <li>Based on the novel representation, we exploit a compact single-stage differentiable network, termed as AdaptivePose. Specifically, we introduce a novel Part Perception Module to perceive the human parts by regressing seven human-part related points. By manipulating human-part related points, we further propose the Enhanced Centeraware Branch to more precisely perceive the human center and the Two-hop Regression Branch together with the Skeleton-Aware Regression Loss to precisely regress the keypoints.</li> <li>Our method significantly simplifies the pipeline of existing multi-person pose estimation methods. The effectiveness is demonstrated on both 2D, 3D pose estimation benchmarks. We achieves the best speed-accuracy tradeoffs without complex refinements and post-processes. Furthermore, extended experiments on CrowdPose and MuPoTS-3D clearly verify the generalizability on crowd and 3D scenes.</li> </ul> <h2 id="body-representation">Body Representation</h2> <p>$C_{inst} → {P_{head}, P_{sho}, P_{la}, P_{ra}, P_{hip}, P_{ll}, P_{rl}} → \bf{Joint}$</p> <h3 id="features">Features</h3> <ul> <li>points are predicted by center feature dynamically and <strong>not pre-defined locations</strong></li> <li>instead of only using the root feature to encode the whole pose information, the features of adaptive points are also leveraged to encode keypoint information of different parts more sufficiently</li> <li>built upon the <strong>pixel-wise</strong> keypoint regression framework</li> <li><strong>differentialbe</strong>, without any non-differentiable process</li> </ul> <h2 id="framework">FrameWork</h2> <div align="center"> <img src="assets/img/attachment/AdaptivePoseFramework20230731221909.png" width="70%" height="70%" align="center"/> <img src="assets/img/attachment/AdaptivePoseFramework20230801134203.png" width="70%" height="70%" align="center"/> </div>]]></content><author><name></name></author><category term="paper-reading"/><category term="pose"/><summary type="html"><![CDATA[AdaptivePose]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://visual-pose-lab.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://visual-pose-lab.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://visual-pose-lab.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>